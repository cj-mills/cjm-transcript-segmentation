"""Segmentation service for text decomposition via NLTK plugin"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/services/segmentation.ipynb.

# %% auto #0
__all__ = ['SegmentationService', 'split_segment_at_position', 'merge_text_segments', 'reindex_segments',
           'reconstruct_source_blocks']

# %% ../../nbs/services/segmentation.ipynb #2bd278c9
from typing import List, Dict, Any, Optional
import asyncio

from cjm_plugin_system.core.manager import PluginManager
from cjm_source_provider.models import SourceBlock

from ..models import TextSegment

# %% ../../nbs/services/segmentation.ipynb #be134f4e
class SegmentationService:
    """Service for text segmentation via NLTK plugin."""
    
    def __init__(
        self,
        plugin_manager: PluginManager,  # Plugin manager for accessing text plugin
        plugin_name: str = "cjm-text-plugin-nltk"  # Name of the text processing plugin
    ):
        """Initialize the segmentation service."""
        self._manager = plugin_manager
        self._plugin_name = plugin_name
    
    def is_available(self) -> bool:  # True if plugin is loaded and ready
        """Check if the text processing plugin is available."""
        return self._manager.get_plugin(self._plugin_name) is not None
    
    def ensure_loaded(
        self,
        config: Optional[Dict[str, Any]] = None  # Optional plugin configuration
    ) -> bool:  # True if successfully loaded
        """Ensure the text processing plugin is loaded."""
        if self.is_available():
            return True
        
        # Try to find and load the plugin
        meta = self._manager.get_discovered_meta(self._plugin_name)
        if meta:
            return self._manager.load_plugin(meta, config or {"language": "english"})
        return False
    
    async def split_sentences_async(
        self,
        text: str,  # Text to split into sentences
        source_id: Optional[str] = None,  # Source block ID for traceability
        source_provider_id: Optional[str] = None  # Source provider identifier for traceability
    ) -> List[TextSegment]:  # List of TextSegment objects
        """Split text into sentences asynchronously."""
        if not self.is_available():
            raise RuntimeError(f"Plugin {self._plugin_name} not loaded")
        
        # Execute plugin
        result = await self._manager.execute_plugin_async(
            self._plugin_name,
            action="split_sentences",
            text=text
        )
        
        # Convert spans to TextSegments
        segments = []
        spans = result.get('spans', [])
        
        for idx, span in enumerate(spans):
            segment = TextSegment(
                index=idx,
                text=span['text'],
                source_id=source_id,
                source_provider_id=source_provider_id,
                start_char=span.get('start_char'),
                end_char=span.get('end_char')
            )
            segments.append(segment)
        
        return segments
    
    def split_sentences(
        self,
        text: str,  # Text to split into sentences
        source_id: Optional[str] = None,  # Source block ID for traceability
        source_provider_id: Optional[str] = None  # Source provider identifier for traceability
    ) -> List[TextSegment]:  # List of TextSegment objects
        """Split text into sentences synchronously."""
        return asyncio.get_event_loop().run_until_complete(
            self.split_sentences_async(text, source_id, source_provider_id)
        )
    
    async def split_combined_sources_async(
        self,
        source_blocks: List[SourceBlock]  # Ordered list of source blocks
    ) -> List[TextSegment]:  # Combined list of TextSegments with proper traceability
        """Split multiple source blocks into segments with proper source tracking."""
        all_segments = []
        global_index = 0
        
        for block in source_blocks:
            segments = await self.split_sentences_async(
                text=block.text,
                source_id=block.id,
                source_provider_id=block.provider_id
            )
            
            # Update indices to be globally sequential
            for seg in segments:
                seg.index = global_index
                global_index += 1
                all_segments.append(seg)
        
        return all_segments

# %% ../../nbs/services/segmentation.ipynb #73bdc1d5
def split_segment_at_position(
    segment: TextSegment,  # Segment to split
    char_position: int  # Character position to split at (relative to segment text)
) -> tuple[TextSegment, TextSegment]:  # Two new segments
    """Split a segment into two at the given character position."""
    if char_position <= 0 or char_position >= len(segment.text):
        raise ValueError("Split position must be within segment text")
    
    # Calculate new character offsets if source tracking exists
    first_start = segment.start_char
    first_end = segment.start_char + char_position if segment.start_char is not None else None
    second_start = first_end
    second_end = segment.end_char
    
    first = TextSegment(
        index=segment.index,
        text=segment.text[:char_position].strip(),
        source_id=segment.source_id,
        source_provider_id=segment.source_provider_id,
        start_char=first_start,
        end_char=first_end
    )
    
    second = TextSegment(
        index=segment.index + 1,  # Will need reindexing
        text=segment.text[char_position:].strip(),
        source_id=segment.source_id,
        source_provider_id=segment.source_provider_id,
        start_char=second_start,
        end_char=second_end
    )
    
    return first, second

# %% ../../nbs/services/segmentation.ipynb #5319e45c
def merge_text_segments(
    first: TextSegment,  # First segment (earlier in sequence)
    second: TextSegment,  # Second segment (later in sequence)
    separator: str = " "  # Text separator between segments
) -> TextSegment:  # Merged segment
    """Merge two adjacent segments into one."""
    merged_text = first.text + separator + second.text
    
    # Preserve source tracking if from same source
    source_id = first.source_id if first.source_id == second.source_id else None
    source_provider_id = first.source_provider_id if first.source_provider_id == second.source_provider_id else None
    
    # Merge character ranges if both exist and from same source
    start_char = first.start_char if source_id else None
    end_char = second.end_char if source_id else None
    
    return TextSegment(
        index=first.index,
        text=merged_text,
        source_id=source_id,
        source_provider_id=source_provider_id,
        start_char=start_char,
        end_char=end_char,
    )

# %% ../../nbs/services/segmentation.ipynb #9eb1119f
def reindex_segments(
    segments: List[TextSegment]  # List of segments to reindex
) -> List[TextSegment]:  # Segments with corrected indices
    """Reindex segments to have sequential indices starting from 0."""
    for idx, segment in enumerate(segments):
        segment.index = idx
    return segments

# %% ../../nbs/services/segmentation.ipynb #k84albg4l2
def reconstruct_source_blocks(
    segment_dicts: List[Dict[str, Any]],  # Serialized working segments
) -> List[SourceBlock]:  # Reconstructed source blocks with combined text
    """Reconstruct source blocks by grouping segments by source_id and combining text."""
    segments_by_source: Dict[str, List[Dict[str, Any]]] = {}
    for seg_dict in segment_dicts:
        source_id = seg_dict.get("source_id", "unknown")
        if source_id not in segments_by_source:
            segments_by_source[source_id] = []
        segments_by_source[source_id].append(seg_dict)
    
    source_blocks = []
    for source_id, segs in segments_by_source.items():
        combined_text = " ".join(s.get("text", "") for s in segs)
        source_provider_id = segs[0].get("source_provider_id", "unknown") if segs else "unknown"
        source_blocks.append(SourceBlock(
            id=source_id, provider_id=source_provider_id, text=combined_text,
        ))
    
    return source_blocks
