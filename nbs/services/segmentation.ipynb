{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982573e3",
   "metadata": {},
   "source": [
    "# segmentation\n",
    "\n",
    "> Segmentation service for text decomposition via NLTK plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03dfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp services.segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd278c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Dict, Any, Optional\n",
    "import asyncio\n",
    "\n",
    "from cjm_plugin_system.core.manager import PluginManager\n",
    "from cjm_source_provider.models import SourceBlock\n",
    "\n",
    "from cjm_transcript_segmentation.models import TextSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9ddf3",
   "metadata": {},
   "source": [
    "## SegmentationService\n",
    "\n",
    "This service wraps the NLTK text processing plugin to provide sentence splitting functionality. It converts raw text into `TextSegment` objects for further refinement in the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be134f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SegmentationService:\n",
    "    \"\"\"Service for text segmentation via NLTK plugin.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        plugin_manager: PluginManager,  # Plugin manager for accessing text plugin\n",
    "        plugin_name: str = \"cjm-text-plugin-nltk\"  # Name of the text processing plugin\n",
    "    ):\n",
    "        \"\"\"Initialize the segmentation service.\"\"\"\n",
    "        self._manager = plugin_manager\n",
    "        self._plugin_name = plugin_name\n",
    "    \n",
    "    def is_available(self) -> bool:  # True if plugin is loaded and ready\n",
    "        \"\"\"Check if the text processing plugin is available.\"\"\"\n",
    "        return self._manager.get_plugin(self._plugin_name) is not None\n",
    "    \n",
    "    def ensure_loaded(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None  # Optional plugin configuration\n",
    "    ) -> bool:  # True if successfully loaded\n",
    "        \"\"\"Ensure the text processing plugin is loaded.\"\"\"\n",
    "        if self.is_available():\n",
    "            return True\n",
    "        \n",
    "        # Try to find and load the plugin\n",
    "        meta = self._manager.get_discovered_meta(self._plugin_name)\n",
    "        if meta:\n",
    "            return self._manager.load_plugin(meta, config or {\"language\": \"english\"})\n",
    "        return False\n",
    "    \n",
    "    async def split_sentences_async(\n",
    "        self,\n",
    "        text: str,  # Text to split into sentences\n",
    "        source_id: Optional[str] = None,  # Source block ID for traceability\n",
    "        source_provider_id: Optional[str] = None  # Source provider identifier for traceability\n",
    "    ) -> List[TextSegment]:  # List of TextSegment objects\n",
    "        \"\"\"Split text into sentences asynchronously.\"\"\"\n",
    "        if not self.is_available():\n",
    "            raise RuntimeError(f\"Plugin {self._plugin_name} not loaded\")\n",
    "        \n",
    "        # Execute plugin\n",
    "        result = await self._manager.execute_plugin_async(\n",
    "            self._plugin_name,\n",
    "            action=\"split_sentences\",\n",
    "            text=text\n",
    "        )\n",
    "        \n",
    "        # Convert spans to TextSegments\n",
    "        segments = []\n",
    "        spans = result.get('spans', [])\n",
    "        \n",
    "        for idx, span in enumerate(spans):\n",
    "            segment = TextSegment(\n",
    "                index=idx,\n",
    "                text=span['text'],\n",
    "                source_id=source_id,\n",
    "                source_provider_id=source_provider_id,\n",
    "                start_char=span.get('start_char'),\n",
    "                end_char=span.get('end_char')\n",
    "            )\n",
    "            segments.append(segment)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def split_sentences(\n",
    "        self,\n",
    "        text: str,  # Text to split into sentences\n",
    "        source_id: Optional[str] = None,  # Source block ID for traceability\n",
    "        source_provider_id: Optional[str] = None  # Source provider identifier for traceability\n",
    "    ) -> List[TextSegment]:  # List of TextSegment objects\n",
    "        \"\"\"Split text into sentences synchronously.\"\"\"\n",
    "        return asyncio.get_event_loop().run_until_complete(\n",
    "            self.split_sentences_async(text, source_id, source_provider_id)\n",
    "        )\n",
    "    \n",
    "    async def split_combined_sources_async(\n",
    "        self,\n",
    "        source_blocks: List[SourceBlock]  # Ordered list of source blocks\n",
    "    ) -> List[TextSegment]:  # Combined list of TextSegments with proper traceability\n",
    "        \"\"\"Split multiple source blocks into segments with proper source tracking.\"\"\"\n",
    "        all_segments = []\n",
    "        global_index = 0\n",
    "        \n",
    "        for block in source_blocks:\n",
    "            segments = await self.split_sentences_async(\n",
    "                text=block.text,\n",
    "                source_id=block.id,\n",
    "                source_provider_id=block.provider_id\n",
    "            )\n",
    "            \n",
    "            # Update indices to be globally sequential\n",
    "            for seg in segments:\n",
    "                seg.index = global_index\n",
    "                global_index += 1\n",
    "                all_segments.append(seg)\n",
    "        \n",
    "        return all_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb90eb8",
   "metadata": {},
   "source": [
    "## Segment Manipulation Helpers\n",
    "\n",
    "These functions support the UI operations for splitting, merging, and reordering segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bdc1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_segment_at_position(\n",
    "    segment: TextSegment,  # Segment to split\n",
    "    char_position: int  # Character position to split at (relative to segment text)\n",
    ") -> tuple[TextSegment, TextSegment]:  # Two new segments\n",
    "    \"\"\"Split a segment into two at the given character position.\"\"\"\n",
    "    if char_position <= 0 or char_position >= len(segment.text):\n",
    "        raise ValueError(\"Split position must be within segment text\")\n",
    "    \n",
    "    # Calculate new character offsets if source tracking exists\n",
    "    first_start = segment.start_char\n",
    "    first_end = segment.start_char + char_position if segment.start_char is not None else None\n",
    "    second_start = first_end\n",
    "    second_end = segment.end_char\n",
    "    \n",
    "    first = TextSegment(\n",
    "        index=segment.index,\n",
    "        text=segment.text[:char_position].strip(),\n",
    "        source_id=segment.source_id,\n",
    "        source_provider_id=segment.source_provider_id,\n",
    "        start_char=first_start,\n",
    "        end_char=first_end\n",
    "    )\n",
    "    \n",
    "    second = TextSegment(\n",
    "        index=segment.index + 1,  # Will need reindexing\n",
    "        text=segment.text[char_position:].strip(),\n",
    "        source_id=segment.source_id,\n",
    "        source_provider_id=segment.source_provider_id,\n",
    "        start_char=second_start,\n",
    "        end_char=second_end\n",
    "    )\n",
    "    \n",
    "    return first, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def merge_text_segments(\n",
    "    first: TextSegment,  # First segment (earlier in sequence)\n",
    "    second: TextSegment,  # Second segment (later in sequence)\n",
    "    separator: str = \" \"  # Text separator between segments\n",
    ") -> TextSegment:  # Merged segment\n",
    "    \"\"\"Merge two adjacent segments into one.\"\"\"\n",
    "    merged_text = first.text + separator + second.text\n",
    "    \n",
    "    # Preserve source tracking if from same source\n",
    "    source_id = first.source_id if first.source_id == second.source_id else None\n",
    "    source_provider_id = first.source_provider_id if first.source_provider_id == second.source_provider_id else None\n",
    "    \n",
    "    # Merge character ranges if both exist and from same source\n",
    "    start_char = first.start_char if source_id else None\n",
    "    end_char = second.end_char if source_id else None\n",
    "    \n",
    "    return TextSegment(\n",
    "        index=first.index,\n",
    "        text=merged_text,\n",
    "        source_id=source_id,\n",
    "        source_provider_id=source_provider_id,\n",
    "        start_char=start_char,\n",
    "        end_char=end_char,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reindex_segments(\n",
    "    segments: List[TextSegment]  # List of segments to reindex\n",
    ") -> List[TextSegment]:  # Segments with corrected indices\n",
    "    \"\"\"Reindex segments to have sequential indices starting from 0.\"\"\"\n",
    "    for idx, segment in enumerate(segments):\n",
    "        segment.index = idx\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "np739fnjr8i",
   "metadata": {},
   "source": [
    "## Source Block Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k84albg4l2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reconstruct_source_blocks(\n",
    "    segment_dicts: List[Dict[str, Any]],  # Serialized working segments\n",
    ") -> List[SourceBlock]:  # Reconstructed source blocks with combined text\n",
    "    \"\"\"Reconstruct source blocks by grouping segments by source_id and combining text.\"\"\"\n",
    "    segments_by_source: Dict[str, List[Dict[str, Any]]] = {}\n",
    "    for seg_dict in segment_dicts:\n",
    "        source_id = seg_dict.get(\"source_id\", \"unknown\")\n",
    "        if source_id not in segments_by_source:\n",
    "            segments_by_source[source_id] = []\n",
    "        segments_by_source[source_id].append(seg_dict)\n",
    "    \n",
    "    source_blocks = []\n",
    "    for source_id, segs in segments_by_source.items():\n",
    "        combined_text = \" \".join(s.get(\"text\", \"\") for s in segs)\n",
    "        source_provider_id = segs[0].get(\"source_provider_id\", \"unknown\") if segs else \"unknown\"\n",
    "        source_blocks.append(SourceBlock(\n",
    "            id=source_id, provider_id=source_provider_id, text=combined_text,\n",
    "        ))\n",
    "    \n",
    "    return source_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5uuut6mzv",
   "metadata": {},
   "source": [
    "## Tests\n",
    "\n",
    "The following cells demonstrate the segmentation service and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fbbldls8m",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'The art of war is of vital importance to the state.'\n",
      "First:    'The art of war is' (chars 0-18)\n",
      "Second:   'of vital importance to the state.' (chars 18-51)\n"
     ]
    }
   ],
   "source": [
    "# Test split_segment_at_position\n",
    "segment = TextSegment(\n",
    "    index=0,\n",
    "    text=\"The art of war is of vital importance to the state.\",\n",
    "    source_id=\"job_123\",\n",
    "    source_provider_id=\"test-plugin\",\n",
    "    start_char=0,\n",
    "    end_char=51\n",
    ")\n",
    "\n",
    "# Split at position 18 (after \"The art of war is \")\n",
    "first, second = split_segment_at_position(segment, 18)\n",
    "print(f\"Original: '{segment.text}'\")\n",
    "print(f\"First:    '{first.text}' (chars {first.start_char}-{first.end_char})\")\n",
    "print(f\"Second:   '{second.text}' (chars {second.start_char}-{second.end_char})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6amiiibej8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment 1: 'The art of war'\n",
      "Segment 2: 'is of vital importance to the state.'\n",
      "Merged:    'The art of war is of vital importance to the state.'\n",
      "Char range: 0 - 51\n"
     ]
    }
   ],
   "source": [
    "# Test merge_text_segments\n",
    "seg1 = TextSegment(\n",
    "    index=0,\n",
    "    text=\"The art of war\",\n",
    "    source_id=\"job_123\",\n",
    "    source_provider_id=\"test-plugin\",\n",
    "    start_char=0,\n",
    "    end_char=14,\n",
    ")\n",
    "\n",
    "seg2 = TextSegment(\n",
    "    index=1,\n",
    "    text=\"is of vital importance to the state.\",\n",
    "    source_id=\"job_123\",\n",
    "    source_provider_id=\"test-plugin\",\n",
    "    start_char=15,\n",
    "    end_char=51,\n",
    ")\n",
    "\n",
    "merged = merge_text_segments(seg1, seg2)\n",
    "print(f\"Segment 1: '{seg1.text}'\")\n",
    "print(f\"Segment 2: '{seg2.text}'\")\n",
    "print(f\"Merged:    '{merged.text}'\")\n",
    "print(f\"Char range: {merged.start_char} - {merged.end_char}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z3ov1zcsd0g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reindex:\n",
      "  index=5: 'First'\n",
      "  index=10: 'Second'\n",
      "  index=3: 'Third'\n",
      "\n",
      "After reindex:\n",
      "  index=0: 'First'\n",
      "  index=1: 'Second'\n",
      "  index=2: 'Third'\n"
     ]
    }
   ],
   "source": [
    "# Test reindex_segments\n",
    "segments = [\n",
    "    TextSegment(index=5, text=\"First\"),\n",
    "    TextSegment(index=10, text=\"Second\"),\n",
    "    TextSegment(index=3, text=\"Third\")\n",
    "]\n",
    "\n",
    "print(\"Before reindex:\")\n",
    "for s in segments:\n",
    "    print(f\"  index={s.index}: '{s.text}'\")\n",
    "\n",
    "reindex_segments(segments)\n",
    "\n",
    "print(\"\\nAfter reindex:\")\n",
    "for s in segments:\n",
    "    print(f\"  index={s.index}: '{s.text}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "akx94do8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconstruct_source_blocks tests passed\n"
     ]
    }
   ],
   "source": [
    "# Test reconstruct_source_blocks\n",
    "seg_dicts = [\n",
    "    {\"text\": \"First sentence.\", \"source_id\": \"job_1\", \"source_provider_id\": \"provider_a\"},\n",
    "    {\"text\": \"Second sentence.\", \"source_id\": \"job_1\", \"source_provider_id\": \"provider_a\"},\n",
    "    {\"text\": \"Third sentence.\", \"source_id\": \"job_2\", \"source_provider_id\": \"provider_b\"},\n",
    "]\n",
    "\n",
    "blocks = reconstruct_source_blocks(seg_dicts)\n",
    "assert len(blocks) == 2\n",
    "assert blocks[0].id == \"job_1\"\n",
    "assert blocks[0].provider_id == \"provider_a\"\n",
    "assert blocks[0].text == \"First sentence. Second sentence.\"\n",
    "assert blocks[1].id == \"job_2\"\n",
    "assert blocks[1].text == \"Third sentence.\"\n",
    "\n",
    "# Empty input\n",
    "assert reconstruct_source_blocks([]) == []\n",
    "\n",
    "# Missing source_id defaults to \"unknown\"\n",
    "blocks = reconstruct_source_blocks([{\"text\": \"orphan\"}])\n",
    "assert blocks[0].id == \"unknown\"\n",
    "\n",
    "print(\"reconstruct_source_blocks tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hnpmhisctzl",
   "metadata": {},
   "source": [
    "### SegmentationService with NLTK Plugin\n",
    "\n",
    "These tests require the NLTK plugin to be installed and discoverable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ydiqpy2e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] Discovered manifest: cjm-text-plugin-nltk from /mnt/SN850X_8TB_EXT4/Projects/GitHub/cj-mills/cjm-transcript-segmentation/.cjm/manifests/cjm-text-plugin-nltk.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 1 plugins from /mnt/SN850X_8TB_EXT4/Projects/GitHub/cj-mills/cjm-transcript-segmentation/.cjm/manifests\n",
      "Found plugin: cjm-text-plugin-nltk v0.0.2\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test SegmentationService with NLTK plugin\n",
    "from pathlib import Path\n",
    "from cjm_plugin_system.core.manager import PluginManager\n",
    "\n",
    "# Calculate project root from notebook location (nbs/services/ -> project root)\n",
    "project_root = Path.cwd().parent.parent\n",
    "manifests_dir = project_root / \".cjm\" / \"manifests\"\n",
    "\n",
    "# Create plugin manager with explicit search path\n",
    "manager = PluginManager(search_paths=[manifests_dir])\n",
    "manager.discover_manifests()\n",
    "\n",
    "print(f\"Discovered {len(manager.discovered)} plugins from {manifests_dir}\")\n",
    "\n",
    "# Check if NLTK plugin is available\n",
    "nltk_meta = manager.get_discovered_meta(\"cjm-text-plugin-nltk\")\n",
    "if nltk_meta:\n",
    "    print(f\"Found plugin: {nltk_meta.name} v{nltk_meta.version}\")\n",
    "else:\n",
    "    print(\"NLTK plugin not found - install via plugins.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75nnse341bt",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] Launching worker for cjm-text-plugin-nltk...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cjm-text-plugin-nltk] Starting worker on port 33111...\n",
      "[cjm-text-plugin-nltk] Logs: /home/innom-dt/.cjm/logs/cjm-text-plugin-nltk.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] HTTP Request: GET http://127.0.0.1:33111/health \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cjm-text-plugin-nltk] Worker ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] HTTP Request: POST http://127.0.0.1:33111/initialize \"HTTP/1.1 200 OK\"\n",
      "[PluginManager] Loaded plugin: cjm-text-plugin-nltk\n",
      "[PluginManager] HTTP Request: POST http://127.0.0.1:33111/execute \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugin available: True\n",
      "\n",
      "Split into 3 segments:\n",
      "  [0] chars 0-51: 'The art of war is of vital importance to...'\n",
      "  [1] chars 52-121: 'It is a matter of life and death, a road...'\n",
      "  [2] chars 122-192: 'Hence it is a subject of inquiry which c...'\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Initialize and test SegmentationService\n",
    "if nltk_meta:\n",
    "    # Load the plugin\n",
    "    manager.load_plugin(nltk_meta, {\"language\": \"english\"})\n",
    "    \n",
    "    seg_service = SegmentationService(manager)\n",
    "    print(f\"Plugin available: {seg_service.is_available()}\")\n",
    "    \n",
    "    # Test sentence splitting (use await directly - Jupyter supports top-level await)\n",
    "    test_text = (\n",
    "        \"The art of war is of vital importance to the state. \"\n",
    "        \"It is a matter of life and death, a road either to safety or to ruin. \"\n",
    "        \"Hence it is a subject of inquiry which can on no account be neglected.\"\n",
    "    )\n",
    "    \n",
    "    segments = await seg_service.split_sentences_async(\n",
    "        text=test_text,\n",
    "        source_id=\"test_job\",\n",
    "        source_provider_id=\"test\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nSplit into {len(segments)} segments:\")\n",
    "    for seg in segments:\n",
    "        print(f\"  [{seg.index}] chars {seg.start_char}-{seg.end_char}: '{seg.text[:40]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m75mgctr56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] HTTP Request: POST http://127.0.0.1:33111/execute \"HTTP/1.1 200 OK\"\n",
      "[PluginManager] HTTP Request: POST http://127.0.0.1:33111/execute \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 2 blocks into 5 segments:\n",
      "  [0] source=job_1: 'Sun Tzu said the art of war is vita...'\n",
      "  [1] source=job_1: 'It determines victory or defeat....'\n",
      "  [2] source=job_2: 'Know your enemy....'\n",
      "  [3] source=job_2: 'Know yourself....'\n",
      "  [4] source=job_2: 'A hundred battles, a hundred victor...'\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test split_combined_sources_async with multiple source blocks\n",
    "from cjm_source_provider.models import SourceBlock\n",
    "\n",
    "if nltk_meta and seg_service.is_available():\n",
    "    # Create test source blocks\n",
    "    blocks = [\n",
    "        SourceBlock(\n",
    "            id=\"job_1\",\n",
    "            provider_id=\"provider_a\",\n",
    "            text=\"Sun Tzu said the art of war is vital. It determines victory or defeat.\"\n",
    "        ),\n",
    "        SourceBlock(\n",
    "            id=\"job_2\",\n",
    "            provider_id=\"provider_b\",\n",
    "            text=\"Know your enemy. Know yourself. A hundred battles, a hundred victories.\"\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Use await directly (Jupyter supports top-level await)\n",
    "    all_segments = await seg_service.split_combined_sources_async(blocks)\n",
    "    \n",
    "    print(f\"Combined {len(blocks)} blocks into {len(all_segments)} segments:\")\n",
    "    for seg in all_segments:\n",
    "        print(f\"  [{seg.index}] source={seg.source_id}: '{seg.text[:35]}...'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gizfbfzuuh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[PluginManager] HTTP Request: POST http://127.0.0.1:33111/cleanup \"HTTP/1.1 200 OK\"\n",
      "[PluginManager] Unloaded plugin: cjm-text-plugin-nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plugins unloaded\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Cleanup\n",
    "if nltk_meta:\n",
    "    manager.unload_all()\n",
    "    print(\"Plugins unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a528b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
